import requests
from textblob import TextBlob
import requests
from bs4 import BeautifulSoup
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer


def get_article_text(url):
    # Send GET request to the article URL
    response = requests.get(url)
    if response.status_code == 200:
        # Parse the HTML content of the article
        soup = BeautifulSoup(response.content, "html.parser")

        # Extract the text content of the article
        article_text = ""
        for paragraph in soup.find_all("p"):
            article_text += paragraph.get_text() + " "
        
        return article_text.strip()
    else:
        print(f"Failed to fetch article content from '{url}'. Status code: {response.status_code}")
        return ""
    
    
def analyze_sentiment(text):
    # Create a TextBlob object
    blob = TextBlob(text)
    
    # Get the sentiment polarity (-1 to 1)
    sentiment_polarity = blob.sentiment.polarity
    
    # Determine sentiment label based on polarity
    if sentiment_polarity > 0:
        sentiment_label = "Positive"
    elif sentiment_polarity < 0:
        sentiment_label = "Negative"
    else:
        sentiment_label = "Neutral"
    
    return sentiment_label, sentiment_polarity

def scrape_newsapi(search_term, api_key):
    # Base URL for News API
    base_url = "https://newsapi.org/v2/everything"

    # Parameters for the News API request
    params = {
        "q": search_term,
        "language": "en",  # Restrict to English
        "apiKey": api_key
    }

    # Send GET request to the News API
    response = requests.get(base_url, params=params)

    # Check if request was successful
    if response.status_code == 200:
        # Parse the JSON response
        data = response.json()

        # Extract relevant information from the response
        articles = data.get("articles", [])

        # Extract headlines, summaries, URLs, and dates
        news_data = []
        for article in articles:
            headline = article.get("title", "No headline found")
            summary = article.get("description", "")
            url = article.get("url", "")
            date_time = article.get("publishedAt", "")

            if headline and url:
                article_text = get_article_text(url)
                if article_text:
                    parser = PlaintextParser.from_string(article_text, Tokenizer("english"))
                    summarizer = LsaSummarizer()
                    summary_sentences = summarizer(parser.document, 3) 
                    summary = " ".join(str(sentence) for sentence in summary_sentences)

                    # Extract all links in the article
                    links = extract_links(article_text)

                    news_data.append({"headline": headline, "summary": summary, "urls": links, "date_time": date_time, 'url': url})
                else:
                    print(f"Failed to fetch article content from '{url}'.")
            else:
                print("Failed to extract URL for an article.")

        return news_data
    else:
        # If request failed, print error message
        print(f"Failed to fetch news for '{search_term}'. Status code: {response.status_code}")
        return []


def extract_links(text):
    # Parse the text to find all URLs
    links = []
    soup = BeautifulSoup(text, "html.parser")
    for link in soup.find_all("a", href=True):
        links.append(link["href"])
    return links


def print_article(article):
    print(f"Date/Time: {article['date_time']}")
    print(f"Headline: {article['headline']}")
    print("Summary:")
    for sentence in article['summary'].split(". "):  # Split by sentence endings
        print(f" - {sentence.strip()}")  # Remove leading/trailing whitespaces
    print(f"URLs: {article['urls']}")
    print(f"URL: {article['url']}")
    print()

def print_sentiment(sentiment_label, sentiment_polarity):
    print(f"Sentiment: {sentiment_label} (Polarity: {sentiment_polarity:.2f})")

api_key = "a9a44d097ae748db86084a7d71d72606"  # Replace this with your NewsAPI.org API key
search_term = "apple"  # Example search term
    
news_articles = scrape_newsapi(search_term, api_key)

if news_articles:
    print("===== News Articles =====")
    for idx, article in enumerate(news_articles, start=1):
        print(f"Article {idx}:")
        print_article(article)

        # Analyze sentiment of headline and summary
        headline_sentiment_label, headline_sentiment_polarity = analyze_sentiment(article["headline"])
        summary_sentiment_label, summary_sentiment_polarity = analyze_sentiment(" ".join(article["summary"]))

        print("Sentiment Analysis:")
        print(" - Headline:")
        print_sentiment(headline_sentiment_label, headline_sentiment_polarity)
        print(" - Summary:")
        print_sentiment(summary_sentiment_label, summary_sentiment_polarity)

        print("-" * 50)  # Separator between articles
else:
    print("No news articles found.")
    